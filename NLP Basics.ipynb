{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
      "Requirement already satisfied: six in c:\\users\\aditya\\anaconda3\\envs\\tensor_flow\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Collecting singledispatch (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Aditya\\AppData\\Local\\pip\\Cache\\wheels\\4b\\c8\\24\\b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: singledispatch, nltk\n",
      "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing NLTK packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "restuarant = pd.read_csv(\"User_restaurants_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN\n",
       "3                                 Crust is not good.        0.0\n",
       "4          Not tasty and the texture was just nasty.        0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restuarant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learned that if an electric slicer is used the blade becomes hot enough to start to cook the prosciutto.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "example_text = restuarant[\"Review\"][1]\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stopwords and filtering data using list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'before', 'some', \"wouldn't\", \"needn't\", 'under', 'when', 'out', 'to', 'and', 'they', 'in', \"you've\", 'me', 'ain', 'through', 'after', \"hadn't\", 'can', 'ours', 'their', 'a', 'has', 'for', 'during', 'where', \"mustn't\", 'does', 'our', 'at', 'her', 'all', \"isn't\", 'who', 'had', 'then', 'couldn', 'again', 'once', 'will', 'most', 'have', 'm', 'yourselves', \"you're\", 'am', 'having', 'which', 'should', 'weren', 'doesn', \"you'll\", 'he', 'i', 'this', 'that', 'didn', 'but', 'any', 'other', 'him', \"aren't\", 'yourself', 'by', 'do', \"doesn't\", 'aren', 'yours', 'only', 'ma', 'there', 'be', \"don't\", 'here', 'y', 'don', 'won', 'them', 'she', 'just', 'because', 'above', 'while', 't', \"couldn't\", \"wasn't\", 'll', 'below', 'it', 'very', 'his', 'how', 've', \"that'll\", 'into', 'with', 'hers', 'off', 'what', 'same', 'of', 'been', \"hasn't\", 'not', 'doing', 'up', 'shouldn', 'were', 'from', 'we', 'is', 'you', 'few', 'those', 'was', 'whom', 'than', 'isn', 'shan', 'hasn', 'my', \"won't\", \"should've\", 'mustn', 'themselves', 'are', 'the', 'too', 'an', 'about', 'd', 're', 'mightn', 'if', 'himself', 'no', 'down', 'on', 'until', 'itself', 'both', \"shan't\", 'over', 'so', 'being', 'more', 'did', 'hadn', \"shouldn't\", 'ourselves', 'these', 'myself', 'its', \"weren't\", \"you'd\", 'wouldn', 'why', 'against', 'nor', 'between', 'own', 'as', 'o', 'or', \"mightn't\", \"haven't\", 'each', 'your', 'herself', 'wasn', 'haven', \"she's\", 'theirs', \"it's\", \"didn't\", 'needn', 'such', 'further', 'now', 's'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) ##Selecting the stop words we want\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'that', 'if', 'an', 'electric', 'slicer', 'is', 'used', 'the', 'blade', 'becomes', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(example_text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'electric', 'slicer', 'used', 'blade', 'becomes', 'hot', 'enough', 'start', 'cook', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [word for word in word_tokens if not word in stop_words] \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learn', 'that', 'if', 'an', 'electr', 'slicer', 'is', 'use', 'the', 'blade', 'becom', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "stem_tokens=[stemmer.stem(word) for word in word_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the stemmed sentence using jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "score = jaccard_similarity_score(word_tokens,stem_tokens)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter kernelspec install-self --user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
