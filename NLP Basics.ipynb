{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
      "Requirement already satisfied: six in c:\\users\\aditya\\anaconda3\\envs\\tensor_flow\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Collecting singledispatch (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Aditya\\AppData\\Local\\pip\\Cache\\wheels\\4b\\c8\\24\\b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: singledispatch, nltk\n",
      "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing NLTK packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "restuarant = pd.read_csv(\"User_restaurants_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN\n",
       "3                                 Crust is not good.        0.0\n",
       "4          Not tasty and the texture was just nasty.        0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restuarant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learned that if an electric slicer is used the blade becomes hot enough to start to cook the prosciutto.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "example_text = restuarant[\"Review\"][1]\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stopwords and filtering data using list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'have', 'didn', 'below', 'herself', 'my', 'the', 'when', 'y', 'for', 'how', 'had', 'because', 'between', 'some', 'this', 'themselves', 'a', 'ours', \"you're\", 'shan', 'nor', 'own', \"doesn't\", 'will', 'so', 'mustn', 'same', 'over', 'she', 'doing', 'mightn', 's', 'during', 'we', 'them', \"wasn't\", 'did', 'after', 'why', \"don't\", 'himself', 'yourselves', 've', 'itself', 'into', \"wouldn't\", 'haven', 'now', 'against', 'weren', 'just', 'once', \"you've\", \"aren't\", 'not', 'all', 'be', 'him', 'ma', \"needn't\", 'having', \"hadn't\", 'been', 'yourself', 'these', 'were', 'his', 're', \"haven't\", 'more', 'll', 'theirs', 'no', 'before', 'on', 'only', 'couldn', 'can', 't', 'while', \"couldn't\", 'was', 'from', 'me', 'here', 'hadn', 'their', 'who', \"you'll\", 'do', 'm', 'any', 'about', 'if', 'what', \"mustn't\", 'of', 'has', 'further', 'wouldn', 'aren', 'wasn', 'by', 'hasn', 'very', \"hasn't\", \"weren't\", 'is', 'hers', \"shouldn't\", 'it', \"didn't\", 'whom', 'that', 'again', \"that'll\", 'being', 'those', 'too', 'i', 'he', 'you', 'yours', 'off', 'your', 'in', 'out', 'as', 'to', 'most', 'isn', 'they', 'other', 'and', 'shouldn', 'd', 'ourselves', 'its', 'up', 'or', \"shan't\", 'few', 'above', \"won't\", 'which', \"mightn't\", 'down', 'where', 'does', 'until', \"she's\", 'with', 'each', 'o', 'needn', \"it's\", \"isn't\", 'than', 'then', 'should', 'her', 'through', 'at', 'doesn', 'am', 'but', 'under', \"you'd\", 'don', 'are', 'ain', 'such', 'both', 'won', \"should've\", 'there', 'an', 'our', 'myself'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) ##Selecting the stop words we want\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'that', 'if', 'an', 'electric', 'slicer', 'is', 'used', 'the', 'blade', 'becomes', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(example_text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'electric', 'slicer', 'used', 'blade', 'becomes', 'hot', 'enough', 'start', 'cook', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [word for word in word_tokens if not word in stop_words] \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learn', 'that', 'if', 'an', 'electr', 'slicer', 'is', 'use', 'the', 'blade', 'becom', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "stem_tokens=[stemmer.stem(word) for word in word_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the stemmed sentence using jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "score = jaccard_similarity_score(word_tokens,stem_tokens)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('then', 'RB'),\n",
       " ('therefore', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a function to get all the possible POS tags of NLTK?\n",
    "text = word_tokenize(\"And then therefore it was something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def all_pos_tags():\n",
    "    print(nltk.help.upenn_tagset())\n",
    "\n",
    "all_pos_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'i', 'am', 'getting', 'angry', 'and', 'i', 'want', 'my', 'damn', 'pho']\n"
     ]
    }
   ],
   "source": [
    "#Write a function to remove punctuation in NLTK\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    words = nltk.word_tokenize(s)\n",
    "    words=[word.lower() for word in words if word.isalpha()]\n",
    "    print(words)\n",
    "str1 = restuarant[\"Review\"][12]\n",
    "remove_punctuation(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'was', \"n't\", 'even', 'all', 'that', 'great', 'to', 'begin', 'with', '?']\n",
      "['That', \"n't\", 'even', 'great', 'begin', '?']\n"
     ]
    }
   ],
   "source": [
    "#Write a function to remove stop words in NLTK\n",
    "\n",
    "def remove_stop_words(s):\n",
    "    word_tokens = word_tokenize(s)\n",
    "    print(word_tokens)\n",
    "    filtered_sentence = [word for word in word_tokens if not word in stop_words] \n",
    "    print(filtered_sentence)\n",
    "str1 = restuarant[\"Review\"][20]\n",
    "remove_stop_words(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'was', \"n't\", 'even', 'all', 'that', 'great', 'to', 'begin', 'with', '?']\n"
     ]
    }
   ],
   "source": [
    "#Write a function to tokenise a sentence in NLTK\n",
    "\n",
    "def tokenize_sentence(s):\n",
    "    word_tokens = word_tokenize(s)\n",
    "    print(word_tokens)\n",
    "str1 = restuarant[\"Review\"][20]\n",
    "tokenize_sentence(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a function to check whether the word is a German word or not? https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\n",
    "\n",
    "Write a function to get the human names from the text below: President Abraham Lincoln suspended the writ of habeas corpus in the Civil War. President Franklin D. Roosevelt claimed emergency powers to fight the Great Depression and World War II. President George W. Bush adopted an expansive concept of White House power after 9/11. President Barack Obama used executive action to shield some undocumented immigrants from deportation.\n",
    "\n",
    "Write a function to create a word cloud using Python (with or without NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#jupyter kernelspec install-self --user\n",
    "1. Remove stop words from the content by using NLTK English stop words\n",
    "\n",
    "2. Get the stem by using Stemming\n",
    "\n",
    "3. Get the Similarity between two strings by using Jaccard Similarity\n",
    "\n",
    "4. Write a function to get all the possible POS tags of NLTK?\n",
    "\n",
    "5. Write a function to check whether the word is a German word or not?\n",
    "https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\n",
    "\n",
    "6. Write a function to remove punctuation in NLTK\n",
    "\n",
    "7. Write a function to remove stop words using NLTK\n",
    "\n",
    "8. Write a function to tokenize a single sentence using NLTK\n",
    "\n",
    "9. Write a function to get the human names from the text below:\n",
    "President Abraham Lincoln suspended the writ of habeas corpus in the Civil War. President Franklin D. Roosevelt claimed emergency powers to fight the Great Depression and World War II. President George W. Bush adopted an expansive concept of White House power after 9/11. President Barack Obama used executive action to shield some undocumented immigrants from deportation.\n",
    "\n",
    "10. Write a function to create a word cloud using Python (with or without NLTK)\n",
    "\n",
    "11. How to get alpha numeric characters as tokens in NLTK\n",
    "\n",
    "12. How to remove all punctuation marks and non-alpha numeric characters?\n",
    "\n",
    "13. How to create a new corpus with NLTK?\n",
    "\n",
    "14. How to change the NLTK Download directory in Code?\n",
    "\n",
    "15. Show a small sample with pyStatParser\n",
    "\n",
    "16. How to get phrasses form text entries?\n",
    "\n",
    "17. How to use Chunk Extraction in NLTK?\n",
    "\n",
    "18. UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xcb in position 0: ordinal not in range(128)\n",
    "Fix this issue\n",
    "\n",
    "19. Generate tag from text content\n",
    "\n",
    "20. Show a simple NLTK sample using Stanford NER Algorithm\n",
    "\n",
    "21. How to tokenize a string in NLTK?\n",
    "\n",
    "22. How to replace 1,2,3.. with “1st, 2nd, 3rd”\n",
    "How to generate strings like “1st, 2nd, 3rd ..”\n",
    "\n",
    "23. How to extract number from text:\n",
    "How to get the price from Kijiji or Craiglist content\n",
    "\n",
    "24. How to classify documents into categories\n",
    "\n",
    "25. Identify the language from the text\n",
    "\n",
    "26. Check grammar in the sentence by using Python\n",
    "\n",
    "27. Write a simple example to show dependency parsing in NLTK?\n",
    "\n",
    "28. Identify place in this sentence\n",
    "Bolt was born on 21 August 1986 to parents Wellesley and Jennifer Bolt in Sherwood Content, a small town in Jamaica.\n",
    "He has a brother, Sadiki, and a sister, Sherine.\n",
    "His parents ran the local grocery store in the rural area, and Bolt spent his time playing cricket and football in the street with his brother, later saying, “When I was young, I didn’t really think about anything other than sports.”\n",
    "As the reigning 200 m champion at both the World Youth and World Junior championships, Bolt hoped to take a clean sweep of the world 200 m championships in the Senior World Championships in Paris.\n",
    "\n",
    "29. Find all cities in this page:\n",
    "\n",
    "30. Convert “spamming” to “spam” by using Lemmatizer\n",
    "\n",
    "31. Write a simple code to show Perceptron tagger\n",
    "\n",
    "32. Write an example code to show PunktSentenceTokenizer\n",
    "\n",
    "33. Convert past tense “gave” to present tense “give” by using NLTK\n",
    "\n",
    "34. Write a code to show WordNetLemmatizer\n",
    "\n",
    "35. Write a code show MultiNomial Naive Bayes and NLTK\n",
    "\n",
    "36. Write an example to show NLTK Collocation\n",
    "\n",
    "37. Identify Gender from the given sentence by using NLTK\n",
    "Kelly and John went to meet Ryan and Jenni. But Jenni was not there when they reached the place.\n",
    "\n",
    "38. Write a code showcase FreqDist in python\n",
    "\n",
    "39. Do a sentiment analysis on thie sentence\n",
    "I love this sandwich\n",
    "\n",
    "40. Collect nouns from this sentence\n",
    "I am John from Toronto\n",
    "\n",
    "41. Compute N Grams in this sentence\n",
    "Bolt was born on 21 August 1986 to parents Wellesley and Jennifer Bolt in Sherwood Content, a small town in Jamaica.\n",
    "He has a brother, Sadiki, and a sister, Sherine.\n",
    "His parents ran the local grocery store in the rural area, and Bolt spent his time playing cricket and football in the street with his brother, later saying, “When I was young, I didn’t really think about anything other than sports.”\n",
    "As the reigning 200 m champion at both the World Youth and World Junior championships, Bolt hoped to take a clean sweep of the world 200 m championships in the Senior World Championships in Paris.\n",
    "\n",
    "42. Write a code to use WordNik\n",
    "\n",
    "43. Find meaning of “Dunk” by using Cambring API\n",
    "https://dictionary-api.cambridge.org/\n",
    "\n",
    "44. How to count the frequency of bigram?\n",
    "Use the sentence below\n",
    "I love Canada . I am so in love with Canada . Canada is great . samsung is great . I really really love Canadian cities. America can never beat Canada . Canada is better than America\n",
    "\n",
    "45. Count the verbs in this sentence:\n",
    "Bolt was born on 21 August 1986 to parents Wellesley and Jennifer Bolt in Sherwood Content, a small town in Jamaica.\n",
    "He has a brother, Sadiki, and a sister, Sherine.\n",
    "His parents ran the local grocery store in the rural area, and Bolt spent his time playing cricket and football in the street with his brother, later saying, “When I was young, I didn’t really think about anything other than sports.”\n",
    "As the reigning 200 m champion at both the World Youth and World Junior championships, Bolt hoped to take a clean sweep of the world 200 m championships in the Senior World Championships in Paris.\n",
    "\n",
    "46. Show a simple example using ANTLR\n",
    "https://www.antlr.org/\n",
    "\n",
    "47. Simple example to show Latent Semantic Modeling in NLTK\n",
    "\n",
    "48. Show an exmple for Singular Value Decomposition\n",
    "\n",
    "49. Use LDA in NLTK\n",
    "\n",
    "50. Show a simple example using NLTK GAE\n",
    "https://github.com/rutherford/nltk-gae\n",
    "\n",
    "51. Show a simple exaple using Penn Treebank\n",
    "\n",
    "52. Do a simple Sentiment Analysis with NLTK and Naive Bayes\n",
    "\n",
    "53. Write a code to get top terms with TF-IDF score\n",
    "\n",
    "54. Compare two sentences\n",
    "\n",
    "55. Find the probability of a word in a given sentence\n",
    "\n",
    "56. Compare “car” and “äutomobile” by using Wordnet\n",
    "\n",
    "57. Check the frequency of similar words\n",
    "\n",
    "58. Show an example using MaltParser\n",
    "\n",
    "59. Identify the subject of the sentence\n",
    "\n",
    "60. How to extract word from Synset? Show an example\n",
    "\n",
    "61. Generate sentences by using\n",
    "\n",
    "62. Print random sentences like in Lorem Ipsum\n",
    "\n",
    "63. Count ngrams in the sentence below:\n",
    "Bolt was born on 21 August 1986 to parents Wellesley and Jennifer Bolt in Sherwood Content, a small town in Jamaica.\n",
    "He has a brother, Sadiki, and a sister, Sherine.\n",
    "His parents ran the local grocery store in the rural area, and Bolt spent his time playing cricket and football in the street with his brother, later saying, “When I was young, I didn’t really think about anything other than sports.”\n",
    "As the reigning 200 m champion at both the World Youth and World Junior championships, Bolt hoped to take a clean sweep of the world 200 m championships in the Senior World Championships in Paris.\n",
    "\n",
    "64. Use MaltParser in Python\n",
    "\n",
    "65. Generate tags on any celebrity’s tweets?\n",
    "\n",
    "66. Write a code to show TF-IDF\n",
    "\n",
    "67. Classify moview reviews as “good” or “bad” by using NLTK\n",
    "\n",
    "68. Show a sample for Alignment model in NLTK\n",
    "\n",
    "69. How to save an alignment model in NLTK?\n",
    "\n",
    "70. Find verbs in the given sentence\n",
    "Bolt was born on 21 August 1986 to parents Wellesley and Jennifer Bolt in Sherwood Content, a small town in Jamaica.\n",
    "He has a brother, Sadiki, and a sister, Sherine.\n",
    "His parents ran the local grocery store in the rural area, and Bolt spent his time playing cricket and football in the street with his brother, later saying, “When I was young, I didn’t really think about anything other than sports.”\n",
    "As the reigning 200 m champion at both the World Youth and World Junior championships, Bolt hoped to take a clean sweep of the world 200 m championships in the Senior World Championships in Paris.\n",
    "\n",
    "71. Get Named Entities from the sentence\n",
    "Bolt was born on 21 August 1986 to parents Wellesley and Jennifer Bolt in Sherwood Content, a small town in Jamaica.\n",
    "He has a brother, Sadiki, and a sister, Sherine.\n",
    "His parents ran the local grocery store in the rural area, and Bolt spent his time playing cricket and football in the street with his brother, later saying, “When I was young, I didn’t really think about anything other than sports.”\n",
    "As the reigning 200 m champion at both the World Youth and World Junior championships, Bolt hoped to take a clean sweep of the world 200 m championships in the Senior World Championships in Paris.\n",
    "\n",
    "72. Write a code to use Vader Sentiment Analyzer\n",
    "\n",
    "73. Generat Random text in Python\n",
    "\n",
    "74. Find the cosine similarity of two sentences\n",
    "Julie loves me more than Linda loves me\n",
    "Jane likes me more than Julie loves me\n",
    "\n",
    "75. Show an example using ViterbiParser\n",
    "\n",
    "76. Show a simple sentiment analysis by using Pointwise Mutual Information\n",
    "\n",
    "77. Use NER to find persons\n",
    "\n",
    "78. How to parse multiple sentences using MaltParser?\n",
    "\n",
    "79. Find the tense of the sentence\n",
    "\n",
    "80. How to calculate BLEU score for a sentence\n",
    "\n",
    "81. How to find given word is singular or not?\n",
    "\n",
    "82. How to identify contractions in a given sentence\n",
    "\n",
    "83. Find the similarity between “cheap” and “low price” by using NLTK\n",
    "\n",
    "84. In the given sentence “John” spelled wrong as “Jhon”. How to find biblical names and fix these typos?\n",
    "\n",
    "85. Show an exmple to use Metaphones in NLTK or Python\n",
    "\n",
    "86. Show an example to use FuzzyWuzzy\n",
    "\n",
    "87. How to use Stanford Relation Extractor?\n",
    "\n",
    "88. Show an example to find the named entities\n",
    "\n",
    "89. Find the Sentiment by using SentiWordNet\n",
    "\n",
    "90. Create a custom Corpus by using NLTK?\n",
    "\n",
    "91. Show an example to extract relationships using NLTK?\n",
    "\n",
    "92. Fix this code issue\n",
    "\n",
    "93. Identify short form in the sentence by using NLTK\n",
    "“ty. U did an awesome job. However, It would b gr8 If you type w/o short forms”\n",
    "\n",
    "94. How to auto label the given texts\n",
    "\n",
    "95. Identify food names in the given sentence\n",
    "I had some Chips and Turkey for the lunch. Later I had some ice cream and rice in the evening.\n",
    "\n",
    "96. Find the specific phrase by using NLTK Regex\n",
    "The pizza was awesome and brilliant\n",
    "\n",
    "97. Convert these lines to two sentences by using Sentence tokenizer?\n",
    "Fig. 2 shows a U.S.A. map. However, this is not exactly right\n",
    "\n",
    "98. How to convert this positive sentence to negative\n",
    "Anna is a great girl and she learn things quickly.\n",
    "\n",
    "99. Find the general synonyms\n",
    "\n",
    "100. Show an example to extract useful sentences by using NLTK\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
